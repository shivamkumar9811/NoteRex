<<<<<<< HEAD
# NoteRex ğŸ§ âœ¨

**AI-Powered Note Generation Platform**

Transform audio, video, PDFs, and text into intelligent, structured summaries using cutting-edge AI technology.

## ğŸš€ Features

### Multi-Format Input Support
- **Audio Files**: MP3, WAV, AAC, etc.
- **Video Files**: MP4, AVI, MOV, etc.
- **PDF Documents**: Extract and summarize text
- **Plain Text**: Direct text input or TXT files
- **YouTube URLs**: (Coming soon)

### AI-Powered Processing
- **Speech-to-Text**: OpenAI Whisper for accurate transcription
- **Intelligent Summarization**: Gemini 2.0 Flash for advanced analysis

### 4 Summary Formats
1. **Bullet-Point Notes**: Key points in concise bullets
2. **Topic-Wise Structure**: Organized by main topics and subtopics
3. **Key Takeaways**: 3-5 most important insights
4. **Q&A for Revision**: Study-ready question-answer pairs

### Smart Storage
- **Firestore Database**: Store all notes, transcripts, and summaries
- **In-Memory Processing**: No file storage costs - process and delete
- **Search Functionality**: Find notes quickly
- **Note Management**: View and delete notes

## ğŸ—ï¸ Tech Stack

### Frontend
- **Next.js 14**: App Router with React Server Components
- **Tailwind CSS**: Modern, responsive styling
- **shadcn/ui**: Beautiful, accessible UI components
- **Lucide Icons**: Clean, consistent iconography

### Backend
- **Next.js API Routes**: Serverless API endpoints
- **OpenAI Whisper**: State-of-the-art speech recognition
- **Google Gemini 2.0**: Advanced text understanding and generation
- **Firebase Firestore**: Scalable NoSQL database

### Libraries
- `openai`: OpenAI API client
- `@google/generative-ai`: Google Gemini API client
- `firebase`: Firebase SDK
- `pdf-parse`: PDF text extraction
- `busboy`: Multipart form data parsing
- `ytdl-core`: YouTube video processing (future feature)
=======
# ğŸ“ Multimodal AI Lecture Summarizer - NoteRex

> **An end-to-end full-stack AI platform that converts videos, audio, PDFs, and text into structured, intelligent study notes using multimodal AI fusion.**

This system understands content the way humans do â€” by **listening to explanations, reading slides/boards, and combining everything into clean, structured knowledge**.

---

## ğŸ§  What Makes This Different?

Most summarizers only work on text or audio.

**This platform fuses multiple modalities:**
- ğŸ¤ Spoken explanations (audio)
- ğŸ‘ï¸ Visual text from slides & boards (OCR)
- ğŸ“ Subtitles (fallback)
- ğŸ“„ PDFs & plain text

â¡ï¸ Result: **More accurate notes, formulas preserved, better learning outcomes.**

---

## ğŸš€ Key Features

### ğŸ”¹ Multi-Format Input
- ğŸ¥ YouTube videos *(user-provided URLs)*
- ğŸ“¹ Uploaded video files
- ğŸ§ Audio files (MP3, WAV, etc.)
- ğŸ“„ PDF documents
- ğŸ“ Plain text input

### ğŸ”¹ Multimodal AI Processing
- **Whisper** for speech-to-text
- **OpenCV + SSIM** for smart frame selection
- **EasyOCR** for slides, formulas & handwritten text
- **Subtitle fallback** when audio/visual is unclear
- **Timestamp-aligned fusion engine**

### ğŸ”¹ AI-Generated Outputs
- ğŸ“Œ Bullet-point notes
- ğŸ§© Topic-wise structured notes
- â­ Key takeaways
- â“ Q&A for revision
- ğŸ§® Formulas & definitions
- â±ï¸ Timestamped topic changes

### ğŸ”¹ Notes Management
- Save notes to Firestore
- Search & view previous notes
- Delete anytime
- Text-only storage (privacy-first)

---

## âš–ï¸ Terms of Service & Privacy Compliance

This project is **TOS-safe and privacy-focused**:

âœ… Temporary media processing only  
âœ… No video/audio storage  
âœ… No redistribution of YouTube content  
âœ… User-provided URLs only  
âœ… Transformative use (notes, summaries)  
âœ… Text-only outputs stored  

> Media files are processed **in memory and deleted immediately** after analysis.

---

## ğŸ—ï¸ System Architecture

```
Frontend (Next.js + Tailwind)
    â†“
API Gateway (Next.js API Routes)
    â†“
Processing Orchestrator
    â†“
Multimodal Pipelines
    â”œâ”€ Audio â†’ Whisper
    â”œâ”€ Video â†’ OpenCV + OCR
    â”œâ”€ Subtitles â†’ Fallback
    â†“
Multimodal Fusion Engine
    â†“
LLM Reasoning (Gemini / GPT)
    â†“
Structured Notes (Markdown)
    â†“
Firestore Storage
```

---

## ğŸ§© Tech Stack

### Frontend
- **Next.js 14 (App Router)**
- React 18
- Next.js
- Tailwind CSS
- shadcn/ui
- Lucide Icons

### Backend
- Next.js API Routes
- Node.js 18+
- FFmpeg

### Multimodal AI
- yt-dlp (YouTube extraction)
- OpenAI Whisper (speech-to-text)
- OpenCV (frame extraction)
- scikit-image (SSIM)
- EasyOCR (visual text)
- YouTubeTranscriptApi

### LLM
- **Google Gemini 2.0 Flash**
- *(Optional)* GPT-4 / GPT-5

### Database & Auth
- Firebase Firestore
- Firebase Auth

### DevOps
- Vercel (frontend & APIs)
- Docker *(for heavy processing â€“ optional)*

---
>>>>>>> f4dea4239c8c01766c8c99bf25852301ae5f8497

## ğŸ“ Project Structure

```
/app
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
<<<<<<< HEAD
â”‚   â”‚   â””â”€â”€ [[...path]]/
â”‚   â”‚       â””â”€â”€ route.js          # All API endpoints
â”‚   â”œâ”€â”€ page.js                   # Main UI component
â”‚   â”œâ”€â”€ layout.js                 # Root layout with metadata
â”‚   â””â”€â”€ globals.css               # Global styles
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ firebase.js               # Firestore configuration
â”‚   â”œâ”€â”€ openai.js                 # OpenAI client setup
â”‚   â””â”€â”€ gemini.js                 # Gemini AI client setup
â”œâ”€â”€ components/
â”‚   â””â”€â”€ ui/                       # shadcn/ui components
â”œâ”€â”€ .env                          # Environment variables
â””â”€â”€ package.json                  # Dependencies
```

## ğŸ”§ Setup Instructions

### 1. Environment Variables

Create or update `.env` file with the following:

```env
# Emergent LLM Key (for OpenAI Whisper & Gemini)
EMERGENT_LLM_KEY=your_emergent_llm_key_here

# Firebase Firestore Configuration
FIREBASE_API_KEY=your_firebase_api_key
FIREBASE_AUTH_DOMAIN=your_project.firebaseapp.com
FIREBASE_PROJECT_ID=your_project_id
FIREBASE_STORAGE_BUCKET=your_project.appspot.com
=======
â”‚   â”‚   â””â”€â”€ [[...path]]/route.js   # Backend APIs
â”‚   â”œâ”€â”€ page.js                   # Main UI
â”‚   â”œâ”€â”€ layout.js                 # Root layout
â”‚   â””â”€â”€ globals.css               # Styles
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ firebase.js               # Firestore config
â”‚   â”œâ”€â”€ gemini.js                 # Gemini client
â”‚   â”œâ”€â”€ whisper.js                # Whisper wrapper
â”œâ”€â”€ components/
â”‚   â””â”€â”€ ui/                       # shadcn/ui components
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ audio_pipeline.py
â”‚   â”œâ”€â”€ video_pipeline.py
â”‚   â”œâ”€â”€ fusion_engine.py
â”‚   â””â”€â”€ ocr_utils.py
â”œâ”€â”€ .env
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/your-username/multimodal-ai-notes.git
cd multimodal-ai-notes
```

### 2ï¸âƒ£ Environment Variables

Create a `.env` file in the root:

```env
# LLM Keys
GOOGLE_API_KEY=your_gemini_api_key
OPENAI_API_KEY=your_openai_key

# Firebase
FIREBASE_API_KEY=your_key
FIREBASE_AUTH_DOMAIN=your_project.firebaseapp.com
FIREBASE_PROJECT_ID=your_project_id
FIREBASE_STORAGE_BUCKET=your_bucket
>>>>>>> f4dea4239c8c01766c8c99bf25852301ae5f8497
FIREBASE_MESSAGING_SENDER_ID=your_sender_id
FIREBASE_APP_ID=your_app_id
```

<<<<<<< HEAD
### 2. Firebase Setup

1. Go to [Firebase Console](https://console.firebase.google.com/)
2. Create a new project
3. Enable **Firestore Database**:
   - Click "Firestore Database" â†’ "Create database"
   - Start in **test mode** (for development)
   - Select your preferred location
4. Get your Firebase config:
   - Project Settings â†’ Your apps â†’ Web app
   - Copy the configuration values to `.env`

### 3. Install Dependencies

```bash
yarn install
```

### 4. Run Development Server

```bash
yarn dev
```

The app will be available at `http://localhost:3000`

## ğŸ¯ How to Use

### Upload & Process Files

1. **Select Input Method**:
   - Click on file type card (Audio, Video, PDF, Text)
   - Or paste text directly
   - Or enter YouTube URL (coming soon)

2. **Process**:
   - Click "Process File" or "Generate Summary"
   - Wait for AI processing (transcription + summarization)

3. **Review Results**:
   - View full transcript
   - See 4 different summary formats
   - Each optimized for different use cases

4. **Save to Notes**:
   - Click "Save to Notes" to store in Firestore
   - Access anytime from "My Notes" tab

### Manage Notes

- **View All Notes**: Switch to "My Notes" tab
- **Search**: Use the search bar to find specific notes
- **Delete**: Click trash icon to remove notes

## ğŸ”„ API Endpoints

### POST `/api/process`
Process files or text and generate summaries

**For file upload:**
- Method: `POST`
- Content-Type: `multipart/form-data`
- Body: `file`, `sourceType`
- Response: `{ success, data: { title, sourceType, transcript, summaries } }`

**For text input:**
- Method: `POST`
- Content-Type: `application/json`
- Body: `{ text, sourceType: 'text' }`
- Response: `{ success, data: { title, sourceType, transcript, summaries } }`

### POST `/api/notes`
Save note to Firestore
- Method: `POST`
- Body: `{ title, sourceType, transcript, summaries }`
- Response: `{ success, data: { ...noteData, firestoreId } }`

### GET `/api/notes`
Fetch all notes or search
- Method: `GET`
- Query params: `?search=query` (optional)
- Response: `{ success, data: [notes] }`

### DELETE `/api/notes/:id`
Delete a note
- Method: `DELETE`
- Path: `/api/notes/{firestoreId}`
- Response: `{ success }`

## ğŸ¨ Design System

### Colors
- **Primary**: Indigo/Purple gradient
- **Accent**: Pink, Teal, Yellow for different features
- **Background**: Soft gradient (indigo-50 â†’ purple-50 â†’ pink-50)

### Components
All UI components are from **shadcn/ui**:
- Button, Card, Input, Textarea
- Tabs, ScrollArea, Badge, Separator
- Toast notifications via Sonner

### Typography
- Font: Inter (Google Fonts)
- Responsive, accessible text sizing

## ğŸ”’ Security & Privacy

### In-Memory Processing
- Files are processed in memory only
- Immediately deleted after transcription
- No file storage = no storage costs or security risks

### Firestore Security
- Currently in test mode (development)
- **Production**: Enable authentication and security rules

Example security rules for production:
```javascript
rules_version = '2';
service cloud.firestore {
  match /databases/{database}/documents {
    match /notes/{noteId} {
      allow read, write: if request.auth != null;
=======
### 3ï¸âƒ£ Install Frontend Dependencies

```bash
npm install
```

### 4ï¸âƒ£ Python Environment (for multimodal processing)

```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows

pip install -r requirements.txt
```

**Required system dependency:**
ğŸ‘‰ Install FFmpeg and add it to PATH.

### 5ï¸âƒ£ Run Development Server

```bash
npm run dev
```

App will run at:

```
http://localhost:3000
```

---

## ğŸ§ª API Endpoints

### `POST /api/process`

Processes uploaded files or text.

**Response:**
```json
{
  "success": true,
  "data": {
    "title": "Lecture Title",
    "transcript": "...",
    "summaries": {
      "bullets": [],
      "topics": [],
      "takeaways": [],
      "qa": []
>>>>>>> f4dea4239c8c01766c8c99bf25852301ae5f8497
    }
  }
}
```

<<<<<<< HEAD
## ğŸš€ Deployment

### Vercel (Recommended)

1. Push code to GitHub
2. Connect to Vercel
3. Add environment variables in Vercel dashboard
4. Deploy!

### Environment Variables for Production
Make sure to add all `.env` variables in your hosting platform:
- `EMERGENT_LLM_KEY`
- `FIREBASE_API_KEY`
- `FIREBASE_AUTH_DOMAIN`
- `FIREBASE_PROJECT_ID`
- `FIREBASE_STORAGE_BUCKET`
- `FIREBASE_MESSAGING_SENDER_ID`
- `FIREBASE_APP_ID`

## ğŸ› ï¸ Troubleshooting

### Server won't start
```bash
# Check logs
tail -n 50 /var/log/supervisor/nextjs.out.log

# Restart server
sudo supervisorctl restart nextjs
```

### Missing dependencies
```bash
yarn install
```

### Firebase errors
- Verify all Firebase env variables are set correctly
- Check Firestore is enabled in Firebase Console
- Ensure security rules allow access

### API errors
- Check `.env` has valid API keys
- Verify Emergent LLM key has access to OpenAI and Gemini
- Check network connectivity

## ğŸ“ Future Enhancements

- [ ] YouTube video processing
- [ ] User authentication
- [ ] Note sharing and collaboration
- [ ] Export notes (PDF, Markdown)
- [ ] Tags and categories
- [ ] Advanced search with filters
- [ ] Mobile app
- [ ] Browser extension
- [ ] Batch processing
- [ ] Custom AI prompts

## ğŸ¤ Contributing

This is an MVP/hackathon project. Feel free to fork and extend!

## ğŸ“„ License

MIT License - feel free to use for your projects!

---

**Built with â¤ï¸ using Next.js, OpenAI Whisper, and Google Gemini 2.0**
=======
### `POST /api/notes`

Save generated notes to Firestore.

### `GET /api/notes`

Fetch or search saved notes.

### `DELETE /api/notes/:id`

Delete a saved note.

---

## ğŸ” Security Notes

- Media files are never stored
- Firestore rules required for production
- Enable Firebase Auth before public deployment

**Example Firestore Rule:**
```
allow read, write: if request.auth != null;
```

---

## ğŸ“Š Performance Tips

- Increase frame interval (5â€“10s) â†’ faster
- Decrease frame interval (2â€“3s) â†’ more accurate
- Disable OCR for low-memory systems

---

## ğŸ¯ Use Cases

- University lectures
- Online courses
- Technical tutorials
- Webinars
- Exam revision
- Self-learning

---

## ğŸ”® Future Enhancements

- [ ] Topic-wise segmentation
- [ ] Quiz generation
- [ ] PDF / Markdown export
- [ ] Batch video processing
- [ ] User collaboration
- [ ] Mobile app
- [ ] Browser extension

---

## ğŸ† Why This Project Stands Out

âœ” Real multimodal AI  
âœ” Full-stack implementation  
âœ” TOS-compliant  
âœ” Scalable architecture  
âœ” Hackathon & production ready

---

## ğŸ“„ License

MIT License  
Free to use, modify, and distribute.

---

## ğŸ™Œ Acknowledgments

- OpenAI Whisper
- Google Gemini
- EasyOCR
- Firebase
- Open-source community

---

**Built with â¤ï¸ for students, educators, and lifelong learners.**  
ğŸ§ âœ¨ Turn content into knowledge.

>>>>>>> f4dea4239c8c01766c8c99bf25852301ae5f8497
